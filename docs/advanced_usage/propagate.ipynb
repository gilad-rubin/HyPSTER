{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuration Propagation\n",
    "\n",
    "Hypster supports nested configurations by providing a `hp.propagate` API to select and override values at different levels.\n",
    "\n",
    "## Using `hp.propagate()`\n",
    "\n",
    "The `hp.propagate()` function allows you to include one configuration within another, propagating selections and overrides.\n",
    "\n",
    "Example:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first define a hypster config function. It can be in the same Jupyter Notebook/Python Module or in a different one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hypster\n",
    "from hypster import HP\n",
    "\n",
    "@hypster.config\n",
    "def my_config(hp: HP):\n",
    "    llm_model = hp.select({'haiku': 'claude-3-haiku-20240307', \n",
    "                           'sonnet': 'claude-3-5-sonnet-20240620',\n",
    "                           'gpt-4o-mini': 'gpt-4o-mini'}, default='gpt-4o-mini')\n",
    "    \n",
    "    llm_config = {'temperature': hp.number_input(0), \n",
    "                  'max_tokens': hp.number_input(64)}\n",
    "    \n",
    "    system_prompt = hp.text_input('You are a helpful assistant. Answer with one word only')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "hypster.save(my_config, \"my_config.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We can then `load` it from its path and have it be part of the parent configuration.\n",
    "- We can select & override values within our nested configuration by using dot notation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'my_config': <hypster.core.Hypster at 0x10ad11390>,\n",
       " 'my_conf': {'llm_model': 'claude-3-haiku-20240307',\n",
       "  'llm_config': {'temperature': 0, 'max_tokens': 64},\n",
       "  'system_prompt': 'You are a helpful assistant. Answer with *two words* only'},\n",
       " 'a': 'd'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from hypster import HP, config\n",
    "\n",
    "@config\n",
    "def my_config_parent(hp: HP):\n",
    "    import hypster\n",
    "    my_config = hypster.load(\"my_config.py\")\n",
    "    my_conf = hp.propagate(my_config)\n",
    "    a = hp.select([\"a\", \"b\", \"c\"], default=\"a\")\n",
    "\n",
    "my_config_parent(selections={\"my_conf.llm_model\": \"haiku\"},\n",
    "                 overrides={\"a\" : \"d\", \n",
    "                            \"my_conf.system_prompt\": \"You are a helpful assistant. Answer with *two words* only\"})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hypster-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
