{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Configuration Snapshots for Reproducibility\n",
                "\n",
                "- Hypster provides a way to capture a snapshot of the configuration for reproducibility purposes.\n",
                "- This is especially useful for reproducibility purposes in Machine Learning & AI projects or any scenario where you need to recreate exact configurations.\n",
                "- When using `hp.propagate`, the resulting snapshot also returns values from nested configurations.\n",
                "\n",
                "## Using `return_config_snapshot=True`\n",
                "\n",
                "When calling a Hypster configuration, you can set `return_config_snapshot=True` to get a dictionary of all instantiated values.\n",
                "\n",
                "Example:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 34,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Overwriting llm_model.py\n"
                    ]
                }
            ],
            "source": [
                "%%writefile llm_model.py\n",
                "\n",
                "#This is a mock class for demonstration purposes\n",
                "class LLMModel:\n",
                "    def __init__(self, chunking, model, config):\n",
                "        self.chunking = chunking\n",
                "        self.model = model\n",
                "        self.config = config\n",
                "    \n",
                "    def __eq__(self, other):\n",
                "        return (self.chunking == other.chunking and\n",
                "                self.model == other.model and\n",
                "                self.config == other.config)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 35,
            "metadata": {},
            "outputs": [],
            "source": [
                "from hypster import HP, config\n",
                "\n",
                "\n",
                "@config\n",
                "def my_config(hp: HP):\n",
                "    from llm_model import LLMModel\n",
                "\n",
                "    chunking_strategy = hp.select([\"paragraph\", \"semantic\", \"fixed\"], default=\"paragraph\")\n",
                "\n",
                "    llm_model = hp.select(\n",
                "        {\"haiku\": \"claude-3-haiku-20240307\", \"sonnet\": \"claude-3-5-sonnet-20240620\", \"gpt-4o-mini\": \"gpt-4o-mini\"},\n",
                "        default=\"gpt-4o-mini\",\n",
                "    )\n",
                "\n",
                "    llm_config = {\"temperature\": hp.number(0), \"max_tokens\": 64}\n",
                "\n",
                "    model = LLMModel(chunking_strategy, llm_model, llm_config)\n",
                "\n",
                "\n",
                "results, snapshot = my_config(selections={\"llm_model\": \"haiku\"}, return_config_snapshot=True)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 36,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "{'chunking_strategy': 'paragraph',\n",
                            " 'llm_model': 'claude-3-haiku-20240307',\n",
                            " 'llm_config': {'temperature': 0, 'max_tokens': 64},\n",
                            " 'model': <llm_model.LLMModel at 0x111c3ff40>}"
                        ]
                    },
                    "execution_count": 36,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "results"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 37,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "{'chunking_strategy': 'paragraph',\n",
                            " 'llm_model': 'claude-3-haiku-20240307',\n",
                            " 'llm_config.temperature': 0}"
                        ]
                    },
                    "execution_count": 37,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "snapshot"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "The difference between the `results` and `snapshot` are subtle, but important:\n",
                "- `results` contains the instantiated results from the selections & overrides of the config function.\n",
                "    - Notice the `'model'` output in the `results` dictionary\n",
                "- `snapshot` contains the values that are necessary to get the exact output by using overrides=snapshot\n",
                "    - Notice that `'model'` isn't found in the snapshot since it is a byproduct of the previous selected parameters (`chunking_strategy`, `llm_model`, etc...)\n",
                "    - Notice that we have `llm_config.temperature` only, since this `max_tokens` isn't a configurable parameter.\n",
                "\n",
                "### Example Usage:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 38,
            "metadata": {},
            "outputs": [],
            "source": [
                "reproduced_results = my_config(overrides=snapshot)\n",
                "assert reproduced_results == results  # This should be True"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "This ensures that you can recreate the exact configuration state, which is crucial for reproducibility in machine learning experiments, ensuring consistent results across multiple runs or different environments.\n",
                "\n",
                "## Nested Configurations\n",
                "\n",
                "When using `hp.propagate`, the snapshot captures the entire hierarchy of configurations:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 41,
            "metadata": {},
            "outputs": [],
            "source": [
                "from hypster import HP, config, save\n",
                "\n",
                "\n",
                "@config\n",
                "def my_config(hp: HP):\n",
                "    llm_model = hp.select(\n",
                "        {\"haiku\": \"claude-3-haiku-20240307\", \"sonnet\": \"claude-3-5-sonnet-20240620\", \"gpt-4o-mini\": \"gpt-4o-mini\"},\n",
                "        default=\"gpt-4o-mini\",\n",
                "    )\n",
                "\n",
                "    llm_config = {\"temperature\": hp.number(0), \"max_tokens\": hp.number(64)}"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 42,
            "metadata": {},
            "outputs": [],
            "source": [
                "save(my_config, \"my_config.py\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "- We can then `load` it from its path and have it be part of the parent configuration.\n",
                "- We can select & override values within our nested configuration by using dot notation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 43,
            "metadata": {},
            "outputs": [],
            "source": [
                "@config\n",
                "def my_config_parent(hp: HP):\n",
                "    import hypster\n",
                "\n",
                "    my_config = hypster.load(\"my_config.py\")\n",
                "    my_conf = hp.propagate(my_config)\n",
                "    a = hp.select([\"a\", \"b\", \"c\"], default=\"a\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 56,
            "metadata": {},
            "outputs": [],
            "source": [
                "final_vars = [\"my_conf\", \"a\"]\n",
                "\n",
                "results, snapshot = my_config_parent(\n",
                "    final_vars=final_vars, selections={\"my_conf.llm_model\": \"haiku\"}, overrides={\"a\": \"d\"}, return_config_snapshot=True\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 57,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "{'my_conf': {'llm_model': 'claude-3-haiku-20240307',\n",
                            "  'llm_config': {'temperature': 0, 'max_tokens': 64}},\n",
                            " 'a': 'd'}"
                        ]
                    },
                    "execution_count": 57,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "results"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 58,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "{'my_conf.llm_model': 'claude-3-haiku-20240307',\n",
                            " 'my_conf.llm_config.temperature': 0,\n",
                            " 'my_conf.llm_config.max_tokens': 64,\n",
                            " 'a': 'd'}"
                        ]
                    },
                    "execution_count": 58,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "snapshot"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 60,
            "metadata": {},
            "outputs": [],
            "source": [
                "reproduced_results = my_config_parent(final_vars=final_vars, overrides=snapshot)\n",
                "assert reproduced_results == results"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "hypster-env",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.14"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
